{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "441fcd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imbalanced-learn installed into c:\\Users\\ILYESS\\Desktop\\training\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# Ensure imbalanced-learn (imblearn) is installed in the notebook kernel environment\n",
    "import sys, subprocess\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'imbalanced-learn'])\n",
    "print('imbalanced-learn installed into', sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6261b955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib installed into c:\\Users\\ILYESS\\Desktop\\training\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# Ensure matplotlib is installed in the notebook kernel environment\n",
    "import sys, subprocess\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'matplotlib'])\n",
    "print('matplotlib installed into', sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b542d222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "X_train shape: (755065, 15547)\n",
      "X_train data bytes: 154149896\n",
      "X_train indices bytes: 77074948\n",
      "X_train indptr bytes: 3020264\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Model for Remote Job Prediction\n",
    "# This notebook trains an XGBoost classifier to predict remote job postings.\n",
    "# It uses advanced text processing (TF-IDF, hashing), feature engineering, and handles class imbalance.\n",
    "# The model is tuned, evaluated, and saved for deployment.\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, FeatureHasher\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, roc_auc_score, f1_score,\n",
    "                             classification_report, precision_recall_curve, auc)\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# LOAD DATA\n",
    "# Read the job postings dataset\n",
    "df = pd.read_csv(\"prepared_jobs_dataset.csv\")\n",
    "\n",
    "# Fill missing values in text columns\n",
    "df[\"skill_text\"] = df[\"skill_text\"].fillna(\"\")\n",
    "df[\"job_title_short\"] = df[\"job_title_short\"].fillna(\"\")\n",
    "df[\"company_name\"] = df[\"company_name\"].fillna(\"\")\n",
    "df[\"CountryName\"] = df[\"CountryName\"].fillna(\"Unknown\")\n",
    "\n",
    "# Combine text fields into raw text\n",
    "df[\"raw_text\"] = (\n",
    "    df[\"job_title_short\"].astype(str) + \" \" +\n",
    "    df[\"company_name\"].astype(str) + \" \" +\n",
    "    df[\"skill_text\"].astype(str)\n",
    " )\n",
    "\n",
    "# Clean text: lowercase, remove non-alphanumeric, normalize spaces\n",
    "def clean(t):\n",
    "    t = str(t).lower()\n",
    "    t = re.sub(r\"[^a-z0-9\\\\s]\", \" \", t)\n",
    "    return re.sub(r\"\\\\s+\", \" \", t).strip()\n",
    "\n",
    "df[\"clean_text\"] = df[\"raw_text\"].apply(clean)\n",
    "\n",
    "# NUMERIC FEATURES\n",
    "# Create additional numeric features for better modeling\n",
    "df[\"num_skills\"] = df[\"skill_text\"].apply(lambda x: len(x.split()))\n",
    "df[\"title_len\"] = df[\"job_title_short\"].apply(lambda x: len(x.split()))\n",
    "df[\"unique_words\"] = df[\"clean_text\"].apply(lambda x: len(set(x.split())))\n",
    "df[\"avg_word_len\"] = df[\"clean_text\"].apply(lambda x: np.mean([len(w) for w in x.split()]) if len(x.split()) else 0)\n",
    "\n",
    "# TARGET\n",
    "# Define target variable: remote flag\n",
    "if \"remote_flag\" not in df.columns:\n",
    "    df[\"remote_flag\"] = df[\"job_work_from_home\"].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "y = df[\"remote_flag\"].values\n",
    "\n",
    "# TF-IDF\n",
    "# Vectorize cleaned text with TF-IDF\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=15000,\n",
    "    ngram_range=(1,3),\n",
    "    min_df=3,\n",
    "    max_df=0.9\n",
    ")\n",
    "X_text = tfidf.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "# COUNTRY OHE\n",
    "# One-hot encode country\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "X_country = ohe.fit_transform(df[[\"CountryName\"]])\n",
    "\n",
    "# HASHING\n",
    "# Use feature hashing for company and title to reduce dimensionality\n",
    "fh_company = FeatureHasher(n_features=256, input_type=\"dict\")\n",
    "X_company = fh_company.transform(df[\"company_name\"].apply(lambda s: {\"company\": str(s)}))\n",
    "\n",
    "fh_title = FeatureHasher(n_features=128, input_type=\"dict\")\n",
    "X_title = fh_title.transform(df[\"job_title_short\"].apply(lambda s: {\"title\": str(s)}))\n",
    "\n",
    "# NUMERIC SCALING\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "num_arr = scaler.fit_transform(df[[\"num_skills\",\"title_len\",\"unique_words\",\"avg_word_len\"]])\n",
    "X_num = csr_matrix(num_arr)\n",
    "\n",
    "# COMBINE FEATURES\n",
    "# Horizontally stack all feature matrices\n",
    "X = hstack([X_text, X_country, X_company, X_title, X_num], format=\"csr\")\n",
    "\n",
    "# BALANCE CLASSES\n",
    "# Oversample minority class to balance\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_res, y_res = ros.fit_resample(X, y)\n",
    "\n",
    "# SPLITS\n",
    "# Split into train/validation/test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.15, stratify=y_res, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.15, stratify=y_train_val, random_state=42\n",
    ")\n",
    "\n",
    "# FINAL MODEL\n",
    "# Define XGBoost parameters (pre-tuned)\n",
    "final_params = {\n",
    "    \"n_estimators\": 200,\n",
    "    \"max_depth\": 6,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 1.0,\n",
    "    \"gamma\": 0,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"n_jobs\": 1,\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"use_label_encoder\": False,\n",
    "    \"max_bin\": 256\n",
    "}\n",
    "\n",
    "# Print simple diagnostics before training\n",
    "try:\n",
    "    print('X_train type:', type(X_train))\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    if hasattr(X_train, 'data'):\n",
    "        print('X_train data bytes:', X_train.data.nbytes)\n",
    "        print('X_train indices bytes:', X_train.indices.nbytes)\n",
    "        print('X_train indptr bytes:', X_train.indptr.nbytes)\n",
    "    else:\n",
    "        try:\n",
    "            print('X_train nbytes:', X_train.nbytes)\n",
    "        except Exception:\n",
    "            pass\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TRAIN FINAL MODEL (with basic error handling)\n",
    "# Train XGBoost with early stopping\n",
    "final_model = XGBClassifier(**final_params)\n",
    "try:\n",
    "    final_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print('\\nTraining failed with exception: ', str(e))\n",
    "    print('\\nSuggestions to fix:')\n",
    "    print('- Reduce TF-IDF `max_features` (e.g., 5000-8000)')\n",
    "    print('- Reduce `n_estimators` (e.g., 50-200) and/or `max_depth` (e.g., 4-6)')\n",
    "    print(\"- Set `n_jobs` to 1 (already set) or to a smaller value if you used -1\")\n",
    "    print('- Train on a subset of the data (df.sample(frac=0.2)) to iterate quickly')\n",
    "    print('- Consider not using RandomOverSampler if it densifies the matrix (it may convert sparse to dense)')\n",
    "    raise\n",
    "\n",
    "joblib.dump(final_model, \"xgb_remote_final.joblib\")\n",
    "\n",
    "\n",
    "# THRESHOLD TUNING\n",
    "# Tune decision threshold for better F1 score\n",
    "val_probs = final_model.predict_proba(X_val)[:,1]\n",
    "\n",
    "best_f1, best_t = -1, 0.5\n",
    "for t in np.linspace(0.1, 0.9, 41):\n",
    "    f1v = f1_score(y_val, (val_probs >= t).astype(int))\n",
    "    if f1v > best_f1:\n",
    "        best_f1, best_t = f1v, t\n",
    "\n",
    "test_probs = final_model.predict_proba(X_test)[:,1]\n",
    "test_preds = (test_probs >= best_t).astype(int)\n",
    "\n",
    "print(\"\\nFinal Threshold:\", best_t)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, test_preds))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, test_probs))\n",
    "print(classification_report(y_test, test_preds))\n",
    "\n",
    "\n",
    "# FEATURE IMPORTANCE\n",
    "# Plot top feature importances\n",
    "tfidf_names = tfidf.get_feature_names_out()\n",
    "country_names = ohe.get_feature_names_out([\"CountryName\"])\n",
    "company_names = np.array([f\"company_{i}\" for i in range(X_company.shape[1])])\n",
    "title_names = np.array([f\"title_{i}\" for i in range(X_title.shape[1])])\n",
    "num_names = np.array([\"num_skills\",\"title_len\",\"unique_words\",\"avg_word_len\"])\n",
    "\n",
    "feature_names = np.concatenate([tfidf_names, country_names, company_names, title_names, num_names])\n",
    "importances = final_model.feature_importances_\n",
    "\n",
    "idx = np.argsort(importances)[-25:]\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.barh(range(25), importances[idx])\n",
    "plt.yticks(range(25), feature_names[idx])\n",
    "plt.title(\"Top 25 Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"DONE âœ”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc3175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# SAVE ARTIFACTS FOR APP\n",
    "# -------------------------\n",
    "# Save preprocessing artifacts and alternative model exports for deployment\n",
    "joblib.dump(tfidf, \"tfidf.joblib\")\n",
    "joblib.dump(ohe, \"ohe.joblib\")\n",
    "joblib.dump(scaler, \"scaler.joblib\")\n",
    "# Save XGBoost booster as JSON (compatible with xgboost.Booster)\n",
    "final_model.get_booster().save_model(\"remote_job_model.json\")\n",
    "# Also save the sklearn-wrapped estimator for convenience\n",
    "joblib.dump(final_model, \"remote_job_model.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
