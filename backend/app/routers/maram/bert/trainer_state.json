{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 45000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 6.701421737670898,
      "learning_rate": 1.991155555555556e-05,
      "loss": 0.7536,
      "step": 200
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 6.507566928863525,
      "learning_rate": 1.982266666666667e-05,
      "loss": 0.6902,
      "step": 400
    },
    {
      "epoch": 0.04,
      "grad_norm": 3.227712869644165,
      "learning_rate": 1.973377777777778e-05,
      "loss": 0.6586,
      "step": 600
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 2.5794637203216553,
      "learning_rate": 1.964488888888889e-05,
      "loss": 0.6364,
      "step": 800
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 6.476456642150879,
      "learning_rate": 1.9556e-05,
      "loss": 0.6448,
      "step": 1000
    },
    {
      "epoch": 0.08,
      "grad_norm": 3.420290470123291,
      "learning_rate": 1.946711111111111e-05,
      "loss": 0.6359,
      "step": 1200
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 3.6047210693359375,
      "learning_rate": 1.937822222222222e-05,
      "loss": 0.616,
      "step": 1400
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 3.7646968364715576,
      "learning_rate": 1.9289333333333335e-05,
      "loss": 0.6441,
      "step": 1600
    },
    {
      "epoch": 0.12,
      "grad_norm": 2.5465915203094482,
      "learning_rate": 1.9200444444444446e-05,
      "loss": 0.6363,
      "step": 1800
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 4.190386772155762,
      "learning_rate": 1.9111555555555556e-05,
      "loss": 0.6259,
      "step": 2000
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 6.455027103424072,
      "learning_rate": 1.902266666666667e-05,
      "loss": 0.6323,
      "step": 2200
    },
    {
      "epoch": 0.16,
      "grad_norm": 3.8723437786102295,
      "learning_rate": 1.893377777777778e-05,
      "loss": 0.6294,
      "step": 2400
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 5.057713031768799,
      "learning_rate": 1.884488888888889e-05,
      "loss": 0.5971,
      "step": 2600
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 3.527637243270874,
      "learning_rate": 1.8756e-05,
      "loss": 0.6097,
      "step": 2800
    },
    {
      "epoch": 0.2,
      "grad_norm": 4.1023077964782715,
      "learning_rate": 1.8667111111111115e-05,
      "loss": 0.6074,
      "step": 3000
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 5.183475017547607,
      "learning_rate": 1.8578222222222226e-05,
      "loss": 0.6034,
      "step": 3200
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 4.792686462402344,
      "learning_rate": 1.8489333333333336e-05,
      "loss": 0.61,
      "step": 3400
    },
    {
      "epoch": 0.24,
      "grad_norm": 3.294105291366577,
      "learning_rate": 1.8400444444444447e-05,
      "loss": 0.5837,
      "step": 3600
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 6.551988124847412,
      "learning_rate": 1.8311555555555557e-05,
      "loss": 0.6104,
      "step": 3800
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 4.145142555236816,
      "learning_rate": 1.8222666666666667e-05,
      "loss": 0.5978,
      "step": 4000
    },
    {
      "epoch": 0.28,
      "grad_norm": 4.195614337921143,
      "learning_rate": 1.8133777777777778e-05,
      "loss": 0.5968,
      "step": 4200
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 5.960788726806641,
      "learning_rate": 1.804488888888889e-05,
      "loss": 0.6104,
      "step": 4400
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 2.6758549213409424,
      "learning_rate": 1.7956000000000002e-05,
      "loss": 0.6062,
      "step": 4600
    },
    {
      "epoch": 0.32,
      "grad_norm": 3.7652275562286377,
      "learning_rate": 1.7867111111111113e-05,
      "loss": 0.6013,
      "step": 4800
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 3.742704391479492,
      "learning_rate": 1.7778222222222223e-05,
      "loss": 0.6015,
      "step": 5000
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 4.7671799659729,
      "learning_rate": 1.7689333333333334e-05,
      "loss": 0.591,
      "step": 5200
    },
    {
      "epoch": 0.36,
      "grad_norm": 2.905940532684326,
      "learning_rate": 1.7600444444444447e-05,
      "loss": 0.613,
      "step": 5400
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 4.049108982086182,
      "learning_rate": 1.7511555555555558e-05,
      "loss": 0.5973,
      "step": 5600
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 3.38940691947937,
      "learning_rate": 1.742266666666667e-05,
      "loss": 0.5913,
      "step": 5800
    },
    {
      "epoch": 0.4,
      "grad_norm": 3.6651577949523926,
      "learning_rate": 1.733377777777778e-05,
      "loss": 0.5937,
      "step": 6000
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 3.160158395767212,
      "learning_rate": 1.7244888888888893e-05,
      "loss": 0.6265,
      "step": 6200
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 5.375215530395508,
      "learning_rate": 1.7156000000000003e-05,
      "loss": 0.5816,
      "step": 6400
    },
    {
      "epoch": 0.44,
      "grad_norm": 2.9100983142852783,
      "learning_rate": 1.7067111111111114e-05,
      "loss": 0.5773,
      "step": 6600
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 2.7103476524353027,
      "learning_rate": 1.6978222222222224e-05,
      "loss": 0.6082,
      "step": 6800
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 4.131246566772461,
      "learning_rate": 1.6889333333333334e-05,
      "loss": 0.6065,
      "step": 7000
    },
    {
      "epoch": 0.48,
      "grad_norm": 3.5121190547943115,
      "learning_rate": 1.6800444444444445e-05,
      "loss": 0.5926,
      "step": 7200
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 3.6306705474853516,
      "learning_rate": 1.6711555555555555e-05,
      "loss": 0.5925,
      "step": 7400
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 3.919611692428589,
      "learning_rate": 1.6622666666666666e-05,
      "loss": 0.5621,
      "step": 7600
    },
    {
      "epoch": 0.52,
      "grad_norm": 5.211590766906738,
      "learning_rate": 1.653377777777778e-05,
      "loss": 0.5991,
      "step": 7800
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 3.339376926422119,
      "learning_rate": 1.644488888888889e-05,
      "loss": 0.6043,
      "step": 8000
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 2.485135078430176,
      "learning_rate": 1.6356e-05,
      "loss": 0.605,
      "step": 8200
    },
    {
      "epoch": 0.56,
      "grad_norm": 3.1033031940460205,
      "learning_rate": 1.6267111111111114e-05,
      "loss": 0.5881,
      "step": 8400
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 3.8850324153900146,
      "learning_rate": 1.6178222222222225e-05,
      "loss": 0.6084,
      "step": 8600
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 3.701847791671753,
      "learning_rate": 1.6089333333333335e-05,
      "loss": 0.5441,
      "step": 8800
    },
    {
      "epoch": 0.6,
      "grad_norm": 3.4536826610565186,
      "learning_rate": 1.6000444444444446e-05,
      "loss": 0.5668,
      "step": 9000
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 4.191558837890625,
      "learning_rate": 1.5911555555555556e-05,
      "loss": 0.5627,
      "step": 9200
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 2.1651744842529297,
      "learning_rate": 1.9992e-05,
      "loss": 0.5671,
      "step": 9400
    },
    {
      "epoch": 0.64,
      "grad_norm": 4.068390846252441,
      "learning_rate": 1.990311111111111e-05,
      "loss": 0.5764,
      "step": 9600
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 4.7800493240356445,
      "learning_rate": 1.9814222222222222e-05,
      "loss": 0.5946,
      "step": 9800
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 3.5817434787750244,
      "learning_rate": 1.9725333333333336e-05,
      "loss": 0.5727,
      "step": 10000
    },
    {
      "epoch": 0.68,
      "grad_norm": 2.8819589614868164,
      "learning_rate": 1.9636444444444446e-05,
      "loss": 0.6045,
      "step": 10200
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 2.0714313983917236,
      "learning_rate": 1.9547555555555557e-05,
      "loss": 0.6031,
      "step": 10400
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 3.2569382190704346,
      "learning_rate": 1.9458666666666667e-05,
      "loss": 0.6202,
      "step": 10600
    },
    {
      "epoch": 0.72,
      "grad_norm": 4.367642879486084,
      "learning_rate": 1.9369777777777778e-05,
      "loss": 0.5982,
      "step": 10800
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 2.796466588973999,
      "learning_rate": 1.928088888888889e-05,
      "loss": 0.5842,
      "step": 11000
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 3.596940755844116,
      "learning_rate": 1.9192000000000002e-05,
      "loss": 0.6112,
      "step": 11200
    },
    {
      "epoch": 0.76,
      "grad_norm": 2.514634847640991,
      "learning_rate": 1.9103111111111112e-05,
      "loss": 0.5934,
      "step": 11400
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 2.2949624061584473,
      "learning_rate": 1.9014222222222223e-05,
      "loss": 0.5929,
      "step": 11600
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 2.695470094680786,
      "learning_rate": 1.8925333333333337e-05,
      "loss": 0.5869,
      "step": 11800
    },
    {
      "epoch": 0.8,
      "grad_norm": 2.1573879718780518,
      "learning_rate": 1.8836444444444447e-05,
      "loss": 0.5891,
      "step": 12000
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 2.887798309326172,
      "learning_rate": 1.8747555555555558e-05,
      "loss": 0.5892,
      "step": 12200
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 4.051302909851074,
      "learning_rate": 1.8658666666666668e-05,
      "loss": 0.6007,
      "step": 12400
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.928208827972412,
      "learning_rate": 1.856977777777778e-05,
      "loss": 0.5618,
      "step": 12600
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 2.577641725540161,
      "learning_rate": 1.848088888888889e-05,
      "loss": 0.5794,
      "step": 12800
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 2.790501594543457,
      "learning_rate": 1.8392e-05,
      "loss": 0.569,
      "step": 13000
    },
    {
      "epoch": 0.88,
      "grad_norm": 5.782440662384033,
      "learning_rate": 1.8303111111111113e-05,
      "loss": 0.5858,
      "step": 13200
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 2.0727367401123047,
      "learning_rate": 1.8214222222222224e-05,
      "loss": 0.5904,
      "step": 13400
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 4.02740478515625,
      "learning_rate": 1.8125333333333334e-05,
      "loss": 0.5814,
      "step": 13600
    },
    {
      "epoch": 0.92,
      "grad_norm": 4.387505531311035,
      "learning_rate": 1.8036444444444445e-05,
      "loss": 0.5941,
      "step": 13800
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 3.2015256881713867,
      "learning_rate": 1.794755555555556e-05,
      "loss": 0.5882,
      "step": 14000
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 3.5413904190063477,
      "learning_rate": 1.785866666666667e-05,
      "loss": 0.576,
      "step": 14200
    },
    {
      "epoch": 0.96,
      "grad_norm": 3.997544765472412,
      "learning_rate": 1.776977777777778e-05,
      "loss": 0.5685,
      "step": 14400
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 3.4745209217071533,
      "learning_rate": 1.768088888888889e-05,
      "loss": 0.593,
      "step": 14600
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 4.087258338928223,
      "learning_rate": 1.7592000000000004e-05,
      "loss": 0.5905,
      "step": 14800
    },
    {
      "epoch": 1.0,
      "grad_norm": 5.644825458526611,
      "learning_rate": 1.7503111111111114e-05,
      "loss": 0.6057,
      "step": 15000
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 3.6088366508483887,
      "learning_rate": 1.7414222222222225e-05,
      "loss": 0.5448,
      "step": 15200
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 4.004646301269531,
      "learning_rate": 1.7325333333333335e-05,
      "loss": 0.5652,
      "step": 15400
    },
    {
      "epoch": 1.04,
      "grad_norm": 3.2277252674102783,
      "learning_rate": 1.7236444444444446e-05,
      "loss": 0.5601,
      "step": 15600
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 3.755647897720337,
      "learning_rate": 1.7147555555555556e-05,
      "loss": 0.5328,
      "step": 15800
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 2.777080535888672,
      "learning_rate": 1.7058666666666666e-05,
      "loss": 0.5652,
      "step": 16000
    },
    {
      "epoch": 1.08,
      "grad_norm": 3.9284260272979736,
      "learning_rate": 1.6969777777777777e-05,
      "loss": 0.5333,
      "step": 16200
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 4.336363792419434,
      "learning_rate": 1.688088888888889e-05,
      "loss": 0.5474,
      "step": 16400
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 3.373013496398926,
      "learning_rate": 1.6792e-05,
      "loss": 0.5268,
      "step": 16600
    },
    {
      "epoch": 1.12,
      "grad_norm": 2.2899844646453857,
      "learning_rate": 1.670311111111111e-05,
      "loss": 0.5205,
      "step": 16800
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 3.4683914184570312,
      "learning_rate": 1.6614222222222222e-05,
      "loss": 0.5318,
      "step": 17000
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 5.10565185546875,
      "learning_rate": 1.6525333333333336e-05,
      "loss": 0.536,
      "step": 17200
    },
    {
      "epoch": 1.16,
      "grad_norm": 4.967535018920898,
      "learning_rate": 1.6436444444444446e-05,
      "loss": 0.5281,
      "step": 17400
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 3.150604248046875,
      "learning_rate": 1.6347555555555557e-05,
      "loss": 0.5227,
      "step": 17600
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 3.709704875946045,
      "learning_rate": 1.6258666666666667e-05,
      "loss": 0.554,
      "step": 17800
    },
    {
      "epoch": 1.2,
      "grad_norm": 3.464808702468872,
      "learning_rate": 1.616977777777778e-05,
      "loss": 0.5563,
      "step": 18000
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 4.274908065795898,
      "learning_rate": 1.608088888888889e-05,
      "loss": 0.5206,
      "step": 18200
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 2.7792327404022217,
      "learning_rate": 1.5992000000000002e-05,
      "loss": 0.5555,
      "step": 18400
    },
    {
      "epoch": 1.24,
      "grad_norm": 2.9445509910583496,
      "learning_rate": 1.5903111111111113e-05,
      "loss": 0.5426,
      "step": 18600
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 4.001718044281006,
      "learning_rate": 1.5814222222222223e-05,
      "loss": 0.5557,
      "step": 18800
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 4.98856782913208,
      "learning_rate": 1.5725333333333333e-05,
      "loss": 0.5434,
      "step": 19000
    },
    {
      "epoch": 1.28,
      "grad_norm": 2.940995454788208,
      "learning_rate": 1.5636444444444444e-05,
      "loss": 0.518,
      "step": 19200
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 4.74843168258667,
      "learning_rate": 1.5547555555555554e-05,
      "loss": 0.5292,
      "step": 19400
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 2.3487746715545654,
      "learning_rate": 1.5458666666666668e-05,
      "loss": 0.5335,
      "step": 19600
    },
    {
      "epoch": 1.32,
      "grad_norm": 5.89312744140625,
      "learning_rate": 1.536977777777778e-05,
      "loss": 0.5467,
      "step": 19800
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 9.358243942260742,
      "learning_rate": 1.528088888888889e-05,
      "loss": 0.5301,
      "step": 20000
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 3.235769033432007,
      "learning_rate": 1.5192000000000003e-05,
      "loss": 0.5342,
      "step": 20200
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 4.184683322906494,
      "learning_rate": 1.5103111111111113e-05,
      "loss": 0.5387,
      "step": 20400
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 5.10610818862915,
      "learning_rate": 1.5014222222222224e-05,
      "loss": 0.5648,
      "step": 20600
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 4.297769546508789,
      "learning_rate": 1.4925333333333334e-05,
      "loss": 0.5204,
      "step": 20800
    },
    {
      "epoch": 1.4,
      "grad_norm": 5.465637683868408,
      "learning_rate": 1.4836444444444446e-05,
      "loss": 0.5343,
      "step": 21000
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 3.5471701622009277,
      "learning_rate": 1.4747555555555557e-05,
      "loss": 0.5317,
      "step": 21200
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 6.370585918426514,
      "learning_rate": 1.4658666666666667e-05,
      "loss": 0.5555,
      "step": 21400
    },
    {
      "epoch": 1.44,
      "grad_norm": 4.948476791381836,
      "learning_rate": 1.4569777777777778e-05,
      "loss": 0.5135,
      "step": 21600
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 3.639596462249756,
      "learning_rate": 1.4480888888888892e-05,
      "loss": 0.541,
      "step": 21800
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 5.559946537017822,
      "learning_rate": 1.4392000000000002e-05,
      "loss": 0.5594,
      "step": 22000
    },
    {
      "epoch": 1.48,
      "grad_norm": 6.616770267486572,
      "learning_rate": 1.4303111111111113e-05,
      "loss": 0.5382,
      "step": 22200
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 2.9086663722991943,
      "learning_rate": 1.4214222222222223e-05,
      "loss": 0.5394,
      "step": 22400
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 4.0601301193237305,
      "learning_rate": 1.4125333333333335e-05,
      "loss": 0.5555,
      "step": 22600
    },
    {
      "epoch": 1.52,
      "grad_norm": 5.25908899307251,
      "learning_rate": 1.4036444444444446e-05,
      "loss": 0.5467,
      "step": 22800
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 4.214970588684082,
      "learning_rate": 1.3947555555555556e-05,
      "loss": 0.5267,
      "step": 23000
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 2.569169521331787,
      "learning_rate": 1.3858666666666667e-05,
      "loss": 0.5376,
      "step": 23200
    },
    {
      "epoch": 1.56,
      "grad_norm": 3.7269346714019775,
      "learning_rate": 1.376977777777778e-05,
      "loss": 0.5456,
      "step": 23400
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 4.0434675216674805,
      "learning_rate": 1.3680888888888891e-05,
      "loss": 0.5392,
      "step": 23600
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 5.835258483886719,
      "learning_rate": 1.3592000000000001e-05,
      "loss": 0.5239,
      "step": 23800
    },
    {
      "epoch": 1.6,
      "grad_norm": 4.622193336486816,
      "learning_rate": 1.3503111111111112e-05,
      "loss": 0.5315,
      "step": 24000
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 2.414081573486328,
      "learning_rate": 1.3414222222222224e-05,
      "loss": 0.5443,
      "step": 24200
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 2.9445695877075195,
      "learning_rate": 1.3325333333333334e-05,
      "loss": 0.5116,
      "step": 24400
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 3.6844491958618164,
      "learning_rate": 1.3236444444444445e-05,
      "loss": 0.5377,
      "step": 24600
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 5.0581536293029785,
      "learning_rate": 1.3147555555555555e-05,
      "loss": 0.5327,
      "step": 24800
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 3.4974594116210938,
      "learning_rate": 1.3058666666666669e-05,
      "loss": 0.5267,
      "step": 25000
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 3.7506518363952637,
      "learning_rate": 1.296977777777778e-05,
      "loss": 0.5217,
      "step": 25200
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 3.3414647579193115,
      "learning_rate": 1.288088888888889e-05,
      "loss": 0.5194,
      "step": 25400
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 6.119174003601074,
      "learning_rate": 1.2792e-05,
      "loss": 0.515,
      "step": 25600
    },
    {
      "epoch": 1.72,
      "grad_norm": 3.207029104232788,
      "learning_rate": 1.2703111111111113e-05,
      "loss": 0.5381,
      "step": 25800
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 3.5714468955993652,
      "learning_rate": 1.2614222222222223e-05,
      "loss": 0.5369,
      "step": 26000
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 5.905416965484619,
      "learning_rate": 1.2525333333333334e-05,
      "loss": 0.5262,
      "step": 26200
    },
    {
      "epoch": 1.76,
      "grad_norm": 3.5698273181915283,
      "learning_rate": 1.2436444444444446e-05,
      "loss": 0.5401,
      "step": 26400
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 3.2471208572387695,
      "learning_rate": 1.2347555555555558e-05,
      "loss": 0.5554,
      "step": 26600
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 4.49421501159668,
      "learning_rate": 1.2258666666666668e-05,
      "loss": 0.5368,
      "step": 26800
    },
    {
      "epoch": 1.8,
      "grad_norm": 3.2304611206054688,
      "learning_rate": 1.2169777777777779e-05,
      "loss": 0.5481,
      "step": 27000
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 5.351640224456787,
      "learning_rate": 1.2080888888888891e-05,
      "loss": 0.5411,
      "step": 27200
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 4.1937384605407715,
      "learning_rate": 1.1992000000000001e-05,
      "loss": 0.562,
      "step": 27400
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 5.456979274749756,
      "learning_rate": 1.1903111111111112e-05,
      "loss": 0.525,
      "step": 27600
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 5.400829315185547,
      "learning_rate": 1.1814222222222222e-05,
      "loss": 0.5276,
      "step": 27800
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 3.297374725341797,
      "learning_rate": 1.1725333333333334e-05,
      "loss": 0.5423,
      "step": 28000
    },
    {
      "epoch": 1.88,
      "grad_norm": 7.075802803039551,
      "learning_rate": 1.1636444444444447e-05,
      "loss": 0.5282,
      "step": 28200
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 4.905050277709961,
      "learning_rate": 1.1547555555555557e-05,
      "loss": 0.5376,
      "step": 28400
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 3.402569055557251,
      "learning_rate": 1.1458666666666668e-05,
      "loss": 0.529,
      "step": 28600
    },
    {
      "epoch": 1.92,
      "grad_norm": 4.502429962158203,
      "learning_rate": 1.136977777777778e-05,
      "loss": 0.5539,
      "step": 28800
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 4.963057518005371,
      "learning_rate": 1.128088888888889e-05,
      "loss": 0.5407,
      "step": 29000
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 3.816633939743042,
      "learning_rate": 1.1192e-05,
      "loss": 0.5455,
      "step": 29200
    },
    {
      "epoch": 1.96,
      "grad_norm": 3.2838761806488037,
      "learning_rate": 1.1103111111111111e-05,
      "loss": 0.5664,
      "step": 29400
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 1.9016159772872925,
      "learning_rate": 1.1014222222222223e-05,
      "loss": 0.5277,
      "step": 29600
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 3.110914945602417,
      "learning_rate": 1.0925333333333334e-05,
      "loss": 0.5485,
      "step": 29800
    },
    {
      "epoch": 2.0,
      "grad_norm": 4.971336364746094,
      "learning_rate": 1.0836444444444446e-05,
      "loss": 0.548,
      "step": 30000
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 2.673949718475342,
      "learning_rate": 1.0747555555555556e-05,
      "loss": 0.4689,
      "step": 30200
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 14.4218168258667,
      "learning_rate": 1.0658666666666668e-05,
      "loss": 0.4848,
      "step": 30400
    },
    {
      "epoch": 2.04,
      "grad_norm": 5.7955780029296875,
      "learning_rate": 1.0569777777777779e-05,
      "loss": 0.4578,
      "step": 30600
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 5.316910743713379,
      "learning_rate": 1.048088888888889e-05,
      "loss": 0.4642,
      "step": 30800
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 5.492870330810547,
      "learning_rate": 1.0392e-05,
      "loss": 0.475,
      "step": 31000
    },
    {
      "epoch": 2.08,
      "grad_norm": 3.744722843170166,
      "learning_rate": 1.0303111111111112e-05,
      "loss": 0.4518,
      "step": 31200
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 3.8221850395202637,
      "learning_rate": 1.0214222222222222e-05,
      "loss": 0.4851,
      "step": 31400
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 3.171173095703125,
      "learning_rate": 1.0125333333333333e-05,
      "loss": 0.4805,
      "step": 31600
    },
    {
      "epoch": 2.12,
      "grad_norm": 7.704794883728027,
      "learning_rate": 1.0036444444444445e-05,
      "loss": 0.4701,
      "step": 31800
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 2.9064266681671143,
      "learning_rate": 9.947555555555555e-06,
      "loss": 0.4734,
      "step": 32000
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 6.91480016708374,
      "learning_rate": 9.858666666666668e-06,
      "loss": 0.4991,
      "step": 32200
    },
    {
      "epoch": 2.16,
      "grad_norm": 4.385524749755859,
      "learning_rate": 9.769777777777778e-06,
      "loss": 0.4693,
      "step": 32400
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 10.878378868103027,
      "learning_rate": 9.68088888888889e-06,
      "loss": 0.4819,
      "step": 32600
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 5.242839336395264,
      "learning_rate": 9.592e-06,
      "loss": 0.4504,
      "step": 32800
    },
    {
      "epoch": 2.2,
      "grad_norm": 3.296104907989502,
      "learning_rate": 9.503111111111111e-06,
      "loss": 0.4634,
      "step": 33000
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 8.296554565429688,
      "learning_rate": 9.414222222222223e-06,
      "loss": 0.4934,
      "step": 33200
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 8.331928253173828,
      "learning_rate": 9.325333333333334e-06,
      "loss": 0.4844,
      "step": 33400
    },
    {
      "epoch": 2.24,
      "grad_norm": 7.110760688781738,
      "learning_rate": 9.236444444444446e-06,
      "loss": 0.4389,
      "step": 33600
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 7.769851207733154,
      "learning_rate": 9.147555555555556e-06,
      "loss": 0.4632,
      "step": 33800
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 6.764862537384033,
      "learning_rate": 9.058666666666668e-06,
      "loss": 0.4627,
      "step": 34000
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 3.8156070709228516,
      "learning_rate": 8.969777777777779e-06,
      "loss": 0.4815,
      "step": 34200
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 6.730917453765869,
      "learning_rate": 8.88088888888889e-06,
      "loss": 0.4683,
      "step": 34400
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 7.89198637008667,
      "learning_rate": 8.792e-06,
      "loss": 0.4702,
      "step": 34600
    },
    {
      "epoch": 2.32,
      "grad_norm": 7.23427677154541,
      "learning_rate": 8.703111111111112e-06,
      "loss": 0.4772,
      "step": 34800
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 5.216662883758545,
      "learning_rate": 8.614222222222222e-06,
      "loss": 0.4705,
      "step": 35000
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 6.269798755645752,
      "learning_rate": 8.525333333333335e-06,
      "loss": 0.4857,
      "step": 35200
    },
    {
      "epoch": 2.36,
      "grad_norm": 6.040956497192383,
      "learning_rate": 8.436444444444445e-06,
      "loss": 0.4773,
      "step": 35400
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 9.231579780578613,
      "learning_rate": 8.347555555555557e-06,
      "loss": 0.483,
      "step": 35600
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 9.883322715759277,
      "learning_rate": 8.258666666666668e-06,
      "loss": 0.4611,
      "step": 35800
    },
    {
      "epoch": 2.4,
      "grad_norm": 6.997735500335693,
      "learning_rate": 8.169777777777778e-06,
      "loss": 0.4532,
      "step": 36000
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 6.302754878997803,
      "learning_rate": 8.080888888888889e-06,
      "loss": 0.4445,
      "step": 36200
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 7.832586288452148,
      "learning_rate": 7.992e-06,
      "loss": 0.4791,
      "step": 36400
    },
    {
      "epoch": 2.44,
      "grad_norm": 11.727826118469238,
      "learning_rate": 7.903111111111111e-06,
      "loss": 0.4726,
      "step": 36600
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 6.7451019287109375,
      "learning_rate": 7.814222222222223e-06,
      "loss": 0.4557,
      "step": 36800
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 5.536456108093262,
      "learning_rate": 7.725333333333334e-06,
      "loss": 0.4685,
      "step": 37000
    },
    {
      "epoch": 2.48,
      "grad_norm": 2.24680233001709,
      "learning_rate": 7.636444444444446e-06,
      "loss": 0.4635,
      "step": 37200
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 7.904707431793213,
      "learning_rate": 7.5475555555555556e-06,
      "loss": 0.4712,
      "step": 37400
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 6.427075386047363,
      "learning_rate": 7.458666666666668e-06,
      "loss": 0.4711,
      "step": 37600
    },
    {
      "epoch": 2.52,
      "grad_norm": 8.105268478393555,
      "learning_rate": 7.369777777777778e-06,
      "loss": 0.4601,
      "step": 37800
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 5.342343330383301,
      "learning_rate": 7.2808888888888895e-06,
      "loss": 0.4678,
      "step": 38000
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 9.484210014343262,
      "learning_rate": 7.192e-06,
      "loss": 0.48,
      "step": 38200
    },
    {
      "epoch": 2.56,
      "grad_norm": 5.697020053863525,
      "learning_rate": 7.103111111111112e-06,
      "loss": 0.4341,
      "step": 38400
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 8.938788414001465,
      "learning_rate": 7.0142222222222225e-06,
      "loss": 0.4489,
      "step": 38600
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 8.057605743408203,
      "learning_rate": 6.925333333333334e-06,
      "loss": 0.452,
      "step": 38800
    },
    {
      "epoch": 2.6,
      "grad_norm": 6.863131523132324,
      "learning_rate": 6.836444444444445e-06,
      "loss": 0.4667,
      "step": 39000
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 6.373868942260742,
      "learning_rate": 6.7475555555555564e-06,
      "loss": 0.4672,
      "step": 39200
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 8.696830749511719,
      "learning_rate": 6.658666666666668e-06,
      "loss": 0.4795,
      "step": 39400
    },
    {
      "epoch": 2.64,
      "grad_norm": 4.755051136016846,
      "learning_rate": 6.569777777777778e-06,
      "loss": 0.4748,
      "step": 39600
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 7.585391044616699,
      "learning_rate": 6.4808888888888895e-06,
      "loss": 0.4751,
      "step": 39800
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 8.326733589172363,
      "learning_rate": 6.392000000000001e-06,
      "loss": 0.4726,
      "step": 40000
    },
    {
      "epoch": 2.68,
      "grad_norm": 8.951675415039062,
      "learning_rate": 6.303111111111112e-06,
      "loss": 0.4614,
      "step": 40200
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 7.093532562255859,
      "learning_rate": 6.2142222222222226e-06,
      "loss": 0.4657,
      "step": 40400
    },
    {
      "epoch": 2.7066666666666666,
      "grad_norm": 5.355397701263428,
      "learning_rate": 6.125333333333334e-06,
      "loss": 0.4623,
      "step": 40600
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 7.0012993812561035,
      "learning_rate": 6.036444444444445e-06,
      "loss": 0.4467,
      "step": 40800
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 6.035851955413818,
      "learning_rate": 5.9475555555555565e-06,
      "loss": 0.4777,
      "step": 41000
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 8.013436317443848,
      "learning_rate": 5.858666666666667e-06,
      "loss": 0.4647,
      "step": 41200
    },
    {
      "epoch": 2.76,
      "grad_norm": 6.86245059967041,
      "learning_rate": 5.769777777777778e-06,
      "loss": 0.4771,
      "step": 41400
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 7.861827850341797,
      "learning_rate": 5.6808888888888895e-06,
      "loss": 0.4613,
      "step": 41600
    },
    {
      "epoch": 2.7866666666666666,
      "grad_norm": 6.9780778884887695,
      "learning_rate": 5.592000000000001e-06,
      "loss": 0.4405,
      "step": 41800
    },
    {
      "epoch": 2.8,
      "grad_norm": 3.3794710636138916,
      "learning_rate": 5.503111111111111e-06,
      "loss": 0.4586,
      "step": 42000
    },
    {
      "epoch": 2.8133333333333335,
      "grad_norm": 4.7987260818481445,
      "learning_rate": 5.414222222222223e-06,
      "loss": 0.4794,
      "step": 42200
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 5.059871196746826,
      "learning_rate": 5.325333333333333e-06,
      "loss": 0.4623,
      "step": 42400
    },
    {
      "epoch": 2.84,
      "grad_norm": 7.501466274261475,
      "learning_rate": 5.236444444444445e-06,
      "loss": 0.4526,
      "step": 42600
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 6.774057388305664,
      "learning_rate": 5.147555555555556e-06,
      "loss": 0.4578,
      "step": 42800
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 7.071260929107666,
      "learning_rate": 5.058666666666667e-06,
      "loss": 0.4754,
      "step": 43000
    },
    {
      "epoch": 2.88,
      "grad_norm": 5.803499698638916,
      "learning_rate": 4.969777777777778e-06,
      "loss": 0.4645,
      "step": 43200
    },
    {
      "epoch": 2.8933333333333335,
      "grad_norm": 7.687488555908203,
      "learning_rate": 4.8808888888888896e-06,
      "loss": 0.4583,
      "step": 43400
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 5.93674898147583,
      "learning_rate": 4.792000000000001e-06,
      "loss": 0.4506,
      "step": 43600
    },
    {
      "epoch": 2.92,
      "grad_norm": 8.565496444702148,
      "learning_rate": 4.703111111111111e-06,
      "loss": 0.4899,
      "step": 43800
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 9.098530769348145,
      "learning_rate": 4.614222222222223e-06,
      "loss": 0.4607,
      "step": 44000
    },
    {
      "epoch": 2.9466666666666668,
      "grad_norm": 9.523689270019531,
      "learning_rate": 4.525333333333334e-06,
      "loss": 0.4426,
      "step": 44200
    },
    {
      "epoch": 2.96,
      "grad_norm": 5.238888740539551,
      "learning_rate": 4.436444444444445e-06,
      "loss": 0.4752,
      "step": 44400
    },
    {
      "epoch": 2.9733333333333336,
      "grad_norm": 11.648082733154297,
      "learning_rate": 4.347555555555556e-06,
      "loss": 0.4442,
      "step": 44600
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 4.895857334136963,
      "learning_rate": 4.258666666666667e-06,
      "loss": 0.4577,
      "step": 44800
    },
    {
      "epoch": 3.0,
      "grad_norm": 8.316076278686523,
      "learning_rate": 4.169777777777778e-06,
      "loss": 0.4492,
      "step": 45000
    },
    {
      "epoch": 3.0,
      "step": 45000,
      "total_flos": 2.3748648877338624e+16,
      "train_loss": 0.40859751652611626,
      "train_runtime": 4183.7984,
      "train_samples_per_second": 172.092,
      "train_steps_per_second": 10.756
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.7523833333333333,
      "eval_f1_macro": 0.6825776392384618,
      "eval_loss": 0.6182578206062317,
      "eval_runtime": 94.9621,
      "eval_samples_per_second": 631.831,
      "eval_steps_per_second": 39.489,
      "step": 45000
    }
  ],
  "logging_steps": 200,
  "max_steps": 45000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.3748648877338624e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
